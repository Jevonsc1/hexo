title: 机械学习之svm（转)
date: 2016-08-24 09:37:54
tags: "日常"
---
0.相关概念
------
**分类器：**分类器就是给定一个样本的数据，判定这个样本属于哪个类别的算法。例如在股票涨跌预测中，我们认为前一天的交易量和收盘价对于第二天的涨跌是有影响的，那么分类器就是通过样本的交易量和收盘价预测第二天的涨跌情况的算法。
<!--more-->
**特征：**在分类问题中，输入到分类器中的数据叫做特征。以上面的股票涨跌预测问题为例，特征就是前一天的交易量和收盘价。

**线性分类器：**线性分类器是分类器中的一种，就是判定分类结果的根据是通过**特征**的线性组合得到的，不能通过特征的非线性运算结果作为判定根据。还以上面的股票涨跌预测问题为例，判断的依据只能是前一天的交易量和收盘价的线性组合，不能将交易量和收盘价进行开方，平方等运算。
1.线性分类器起源
------
在实际应用中，我们往往遇到这样的问题：给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。
![图片名称](http://img2.ph.126.net/MM5SEj80Iiy2axDJxIDYMA==/6631571342024130092.png)
怎么分呢？把整个空间劈成两半呗（让我想到了盘古）。用二维空间举个例子，如上图所示，我们用一条直线把空间切割开来，直线左边的点属于类别-1（用三角表示），直线右边的点属于类别1（用方块表示）。

如果用数学语言呢，就是这样的：空间是由X_1和X_2组成的二维空间，直线的方程是$X_1+X_2 = 1$，用向量符号表示即为 $[1,1]^T [X_1,X_2]-1=0$。点x在直线左边的意思是指，当把x放入方程左边，计算结果小于0。同理，在右边就是把x放入方程左边，计算出的结果大于0。都是高中数学知识。

在二维空间中，用一条直线就把空间分割开了：
![图片名称](http://img1.ph.126.net/xm-dmE_iVKPf7r7JhF4_WQ==/6631471286466005209.png)
在三维空间中呢，需要用一个平面把空间切成两半，对应的方程是$X_1+X_2+X_3=1$，也就是 $[1,1,1]^T [X_1,X_2,X_3]-1=0$。
![图片名称](http://img2.ph.126.net/q_z_dskgMpfHft2VwrfXZg==/6631571342024130098.png)
在高维（n>3）空间呢？就需要用到n-1维的超平面将空间切割开了。那么抽象的归纳下：
如果用x表示数据点，用y表示类别（y取1或者-1，代表两个不同的类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面（hyper plane），把空间切割开，这个超平面的方程可以表示为（$ W^T $中的T代表转置）：$$ W^T X + b = 0$$
**感知器模型和逻辑回归：**

常见的线性分类器有感知器模型和逻辑回归。上一节举出的例子是感知器模型，直接给你分好类。有时候，我们除了要知道分类器对于新数据的分类结果，还希望知道分类器对于这次分类的成功概率。逻辑回归就可以做这件事情。

逻辑回归（虽然称作回归，但是不是一个回归方法，却是一个分类算法。很蛋疼的说）将线性分类器的超平面方程计算结果通过logistic函数从正负无穷映射到0到1。这样，映射的结果就可以认为是分类器将x判定为类别1的概率，从而指导后面的学习过程。

举个例子，看天气预报，用感知器的天气预报只会告诉你明天要下雨（y=1），或者明天不下雨（y=-1）；而用了逻辑回归的天气预报就能告诉你明天有90%的概率要下雨，10%的概率不下雨。

逻辑回归的公式是$g(z)=  1\over{1 +e^{-z}}$，图像大概长这个样子：
![图片名称](http://img0.ph.126.net/c4Ouc1kBvuTiuibYlywlZg==/6631590033721803780.png)
怎么用呢？比如感知器模型中，将特征代入判别方程中，如果得到的值是-3，我们可以判定类别是-1（因为-3<0）。而逻辑回归中呢，将-3代入g(z)，我们就知道，该数据属于类别1的概率是0.05（近似数值，谢谢），那么属于类别-1的概率就是1 – 0.05 = 0.95。也就是用概率的观点描述这个事情。

本文深度为“了解”，如果想知道更多的感知器模型和逻辑回归细节，可以参照《统计学习方法》等机器学习的相关书籍。或者持续关注我们的量化课堂，未来会深度探讨。
2.支持向量机 VS 感知器和逻辑回归
------
根据上面的讨论，我们知道了在多维空间下，用一个超平面就把数据分为了两类。这个超平面我们叫它为**分离超平面**。但是这个分离超平面可以有很多个，那么用哪个呢？
![图片名称](http://img1.ph.126.net/xq1HE4W26J9XlgGXzbFtpQ==/6631653805396213792.png)
上图中，对于目前的训练数据，绿色和黑色的直线（二维特征空间，分离超平面就是直线啦）都可以很可以很好的进行分类。但是，通过已知数据建立分离超平面的目的，是为了对于未知数据进行分类的。在下图中，蓝色的星星图案就是新加入的真实数据。
![图片名称](http://img2.ph.126.net/c1VjLhkhm6mjcrisYP3w1w==/6631639511745054191.png)
这时候我们就可以看出不同的分离超平面的选择对于分类效果的影响了。有的绿线会将三个点都划归蓝色圆圈，有的绿线会将三个点都划归红色正方形。

那么绿线和黑线留下谁？我们认为，已有的训练数据中，每个元素距离分离超平面都有一个距离。在添加超平面的时候，尽可能的使**最靠近分离超平面的那个元素**与超平面的距离变大。这样，加入新的数据的时候，分的准的概率会最大化。感知器模型和逻辑回归都不能很好的完成这个工作，该我们的支持向量机（support vector machine，**SVM**）出场了。

首先，SVM将函数间隔（$|W^T X+b|$，将特征值代入分离超平面的方程中，得到的绝对值）归一化，归一化的目的是除掉取值尺度的影响；其次，对所有元素求到超平面的距离，（这个距离是$|W^T X+b|\over|W|$，也就是几何间隔）。给定一个超平面$P$，所有样本距离超平面P的距离可以记为$d_{ij}$，这其中最小的距离记为$D_P$，SVM的作用就是找到$D_P$最大的超平面。

可以看出，大部分数据对于分离超平面都没有作用，能决定分离超平面的，只是已知的训练数据中很小的一部分。这与逻辑回归有非常大的区别。上图中，决定黑色的这条最优分离超平面的数据只有下方的两个红色的数据点和上方的一个蓝色的数据点。这些对于分离超平面有着非常强大影响的数据点也被称为支持向量（看没看到，这就是传说中的支持向量啦，原来如此）。
3.引入黑科技-核函数
------
上面说的都是在原始特征的维度上，能直接找到一条分离超平面将数据完美的分成两类的情况。但如果找不到呢？

比如，原始的输入向量是一维的，$0 < X < 1$的类别是1，其他情况记做-1。这样的情况是不可能在1维空间中找到分离超平面的（一维空间中的分离超平面是一个点，$X+b=0$）。你用一个点切一下试试？
![图片名称](http://img1.ph.126.net/g9i2SX1-drw2imods7XGoA==/6631698885372952540.png)
这就要说到SVM的黑科技—核函数技巧。核函数可以将原始特征映射到另一个高维特征空间中，解决原始空间的线性不可分问题。

继续刚才那个数轴。
![图片名称](http://img1.ph.126.net/ulfGYEZGEgEgCulgYcqdKA==/6631548252279947485.png)
如果我们将原始的一维特征空间映射到二维特征空间$X^2$和$X$，那么就可以找到分离超平面$X^2-X=0$。当$X^2-X<0$的时候，就可以判别为类别1，当$X^2-X>0$的时候，就可以判别为类别0。如下图：
![图片名称](http://img1.ph.126.net/MusR1jWJBgiymjF4HZRT1A==/6631538356675298290.png)
再将$X^2-X=0$映射回原始的特征空间，就可以知道在0和1之间的实例类别是1，剩下空间上（小于0和大于1）的实例类别都是0啦。
![图片名称](http://img2.ph.126.net/uoIL4OcZLzV-tBA6DF2CnQ==/6631698885372952541.png)
利用特征映射，就可以将低维空间中的线性不可分问题解决了。是不是很神奇，这就是特征映射的牛逼之处了。核函数除了能够完成特征映射，而且还能把特征映射之后的内积结果直接返回，大幅度降低了简化了工作，这就是为啥采用核函数的原因。

（为啥要返回数据间的内积结果涉及到比较高深的内容，在此先略过，可以在《统计学习方法》等更加专业的资料中自行阅读，未来量化课堂会产生这部分的内容。）
4.异常值的处理-松弛变量的引入
------
你以为就结束了吗？并没有。

在原始空间线性不可分时，可以映射到高维空间之后，转换为线性可分的问题。但是万一映射之后还是不能线性可分，该如何处理呢？

再比如正常的数据中混入了异常数据，很有可能会使应该的最佳分离超平面移位，或者直接使数据变得线性不可分。又怎么办捏？
![图片名称](http://img1.ph.126.net/DRxOGrCEt_s68j0WvcYAZw==/6631514167419486427.png)
上图中用黑色的圆圈圈起来的就是一个异常值，这个异常值的存在，使得分离超平面发生了移位。这时候就该引入松弛变量了。松弛变量可以允许某些数据点在不满足分离超平面两边的类别要求，从而使得某些严格线性不可分的数据集也可以使用SVM进行分类了。

由于篇幅有限，这部分内容只是为了完整性，不做深入讨论。欢迎关注量化课堂。
5.SVM的具体使用-sklearn
------
SVM的基本原理基本上已经说的差不多了，下面咱们就来看看SVM在实际应用该如何使用了。幸运的是，在python下面，sklearn提供了一个非常好用的机器学习算法，我们调用相关的包就好啦。

在下面的这个例子中，特征是通过收盘价数据计算的SMA，WMA，MOM指标，训练样本的特征是从2007-1-4到2016-6-2中每一天的之前的交易日的收盘价计算的SMA，WMA，MOM指标，训练样本的标签就是2007-1-4日到2016-6-2中每一天的涨跌情况，涨了就是True，跌了就是False，测试样本是2016-6-3日的三个指标以及涨跌情况。我们可以判定之后判断结果是正确还是错误，如果通过SVM判断的结果和当天的涨跌情况相符，则输出True，如果判断结果和当天的涨跌情况不符，则输出False。

我这次的预测结果是输出了True哦。

In[1]

	import talib
	from jqdata import *

	test_stock = '399300.XSHE'
	start_date = datetime.date(2007, 1, 4)
	end_date = datetime.date(2016, 6, 3)

	trading_days = get_all_trade_days()
	start_date_index = trading_days.index(start_date)
	end_date_index = trading_days.index(end_date)

	x_all = []
	y_all = []

	for index in range(start_date_index, end_date_index):
    # 得到计算指标的所有数据
    start_day = trading_days[index - 30]
    end_day = trading_days[index]
    stock_data = get_price(test_stock, start_date=start_day, end_date=end_day, frequency='daily', fields=['close'])
    close_prices = stock_data['close'].values
    
    #通过数据计算指标
    # -2是保证获取的数据是昨天的，-1就是通过今天的数据计算出来的指标
    sma_data = talib.SMA(close_prices)[-2] 
    wma_data = talib.WMA(close_prices)[-2]
    mom_data = talib.MOM(close_prices)[-2]
    
    features = []
    features.append(sma_data)
    features.append(wma_data)
    features.append(mom_data)
    
    label = False
    if close_prices[-1] > close_prices[-2]:
        label = True
    x_all.append(features)
    y_all.append(label)
    

	# 准备算法需要用到的数据
	x_train = x_all[: -1]
	y_train = y_all[: -1]
	x_test = x_all[-1]
	y_test = y_all[-1]
	print('data done')
	
In[]

	from sklearn import svm

	#开始利用机器学习算法计算
	clf = svm.SVC()
	#训练的代码
	clf.fit(x_train, y_train)
	#得到测试结果的代码
	prediction = clf.predict(x_test)

	# 看看预测对了没
	print(prediction == y_test)
	print('all done')	
	